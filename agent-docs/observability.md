# Observability Overview

This document introduces the **observability architecture and implementation** used in this project.  
It is intended for contributors who are new to the codebase and want to understand **how telemetry flows through the system and why it is structured this way**.

---

## Observability Stack

Observability in this project is implemented using the three core telemetry signals:

- **Traces:** Client and Server Side
- **Metrics:** Server Side
- **Logs:** Server Side

Telemetry is instrumented using **OpenTelemetry**. Client side traces are routed through a dedicated collection and forwarding pipeline. Sever side traces metrics and logs are configured using OpenTelemetry

### Storage Backends

Each telemetry signal is stored in a purpose-built backend:

- **Traces** → Grafana Tempo  
- **Metrics** → Prometheus  
- **Logs** → Grafana Loki  

---

## Architecture Overview

Client Side Telemetry generated by the application does **not** flow directly to backends.  
Instead, it is routed through a controlled proxy and collection layer.
Sever Side Telemetry is directly routed to the collection layer.

### High-level Flow

Client
→ Next.js Application
→ OTEL Proxy (API boundary)
→ OpenTelemetry Collector
→ Telemetry Backends

Server
→ Node.js server
→ OpenTelemetry Collector
→ Telemetry Backends

### Signal Routing

- Traces are forwarded to Tempo
- Metrics are scraped or pushed to Prometheus
- Logs are forwarded to Loki

This layered approach provides:
- Centralized authentication and validation
- Failure isolation (telemetry must never break user flows)
- Flexibility to swap or reconfigure backends without application changes

---

## OTEL Proxy (Why it Exists)

The OTEL Proxy acts as a **network, security, and observability boundary**.

Its responsibilities include:
- Validating telemetry tokens
- Enforcing payload size and content-type constraints
- Forwarding valid telemetry to the collector
- Recording its own metrics and traces
- Gracefully handling upstream failures

Importantly:
> **Failures in telemetry pipelines must never affect user-facing functionality.**

---

## OpenTelemetry Collector

The OpenTelemetry Collector is responsible for:
- Receiving telemetry from the OTEL Proxy and Node.js server
- Applying processing and batching
- Forwarding or exposing telemetry to backend systems

The application, proxy and server remain **backend-agnostic** — all backend-specific configuration lives in the collector.

---

## Local vs Production Setup

### Local Development

- A Docker-based setup is used to simulate the full observability stack
- This includes:
  - Collector
  - Tempo
  - Prometheus
  - Loki
  - Grafana

This setup is **strictly for local testing and development**.

> **NOTE:** The Docker setup is not intended for production deployment.

### Production

- Grafana Cloud is used as the managed backend
- The same OpenTelemetry instrumentation and proxy logic are reused
- Only collector/exporter configuration differs

This ensures parity between local and production environments while avoiding vendor lock-in in application code.

---

## Testing & Guarantees

The behavior of the OTEL Proxy is covered by **integration tests** located at:

`/__tests__/integration/otel-proxy.test.ts`

These tests validate the proxy as a system boundary and ensure that:

- Valid telemetry is accepted and forwarded
- Invalid requests are rejected with correct status codes
- Upstream collector failures are handled gracefully
- Telemetry failures do not crash or block the application
- Observability does not cause fan-out or request amplification

All tests run **without requiring a live collector or backend**, ensuring fast and reliable CI execution.

---

## Non-goals

The observability implementation intentionally does **not** aim to:

- Validate OpenTelemetry SDK internals
- Enforce OTLP schema correctness
- Guarantee backend-side ingestion or storage
- Provide performance benchmarking

Those concerns are delegated to backend systems and runtime monitoring.

---

## Summary

This observability architecture prioritizes:

- **Safety** — telemetry must never break application behavior
- **Isolation** — failures are contained within observability boundaries
- **Flexibility** — backends can change without code changes
- **Testability** — all critical behavior is covered by integration tests
