# Welcome

Hi, welcome to LLM Journey, this, as the name of the folder suggests is documentation for humans, written by a human (atleast this file, it feels wierd writing anything on my own, calming for some reason). Although, this file will also probably be parsed through some LLM.

This file documents the exploration and discoveries I made while builing this project. If you want to start contributing using the agentic flow you can go to the [Getting Started Doc](/human-docs/Getting-Started.md)

If you have read the [project readme](/README.md), you know that this poject aims to explore the advancements made in AI industry, from transformer models to agentic system. A lot happened in 2025 if different aspects of the AI industry, but in my opinion the most prominant advancement was made in the Agentic Stack, with Coding Agents leading the way.

# My journey

This project was a way for me to catch to all the progress made. I installed Cursor in Dec 2025 (Yes I know, I am lightyears behind) and used an old repository that was supposed to be something else (don't ask) and started builing.

I had a rough idea of what this project was going to look like when it started, a few pages that will implement various concepts like tool calling, mcps, agentic workflow, etc  with rough explanation. Implemented a bacis home page and a transformers page where a tiny decoder only transformer model (from a Andrej Karpathy vid) hosted using ONNX runtine with the help of the cursor agent and tiny polish here and there from me.

After seeing a few tweets (from Santiago) talking about how most projects using agents will ship garbage because these projects are either not not observable or not writing tests, I though lets include these in the project early so that agents have a strong base to build from. I though this was going to be a one week side mission, after which I will continue building the actual website. Boy I was wrong, I went from trying to implement client side trace from scratch (as recommended by LLMs, reasoning that it will provide more "control"), reviewing the code generated by agent, putting the code changes in chatgpt for review, debugging, reviewing again, using chatgpt again to review, to finally admitting defeat.

(Side Note: I hit my token limits with cursor, switched to google antigravity. Claude code had a lot of hype but I couldn't pass up on the plan google was offering)

# Realization

I was expecting too much from these coding agents. Even thought the project was small, the agent had to figure out what this project was about on every new request. I didn't really know how to get the best out these agents, the only thing I ever prompted was the requirements.
I (with the help of ChatGPT) decided to introduce observablility using OpenTelemetry instrumentation and store the data in grafana. I decided to do this on two parts: Client side observability and Server side observability. Even after the segragating the task, implemetation was a struggle (This was its own journey alone).

For agentic coding to work, you don't only need a good project structure but also need structured instructions for the agents to follow. Documentation that tell agents how to work. Agents will make the same mistakes again and again, there needs to be a record of this, so that agents can avoid repeating mistakes. I heard about AGENTS.md (thanks to 'this day in ai' pod) and thought maybe this is what I was missing. Keeping this in mind I started working on the agent docs.

# Agent Docs

The ideas was to have documentation that would make the agents job easier. I wanted separate agents to handle somewhat independent aspects of the project. This resulted in the creation of the subagents, that handle different aspects of the project like frontend, backend, etc. I also wanted a agent that would help clarify the requirement/specification so that the task to implemented would be crystal clear and no assumptions would be made during the development. This resulted in the creation of the BA agent. Since this replicated real world teams, I created the tech lead to coordinate the sub-agents.

# Feedback / Meta Analysis

The agent docs and workflow became better over time by carrying out feedback or meta analysis after agent performed their task. Below is an example of the feedback prompts I used (I didn't refine these prompts, just didn't feel like the right thing to do):

`Thanks for playing the role of the <role> Agent. Good job on perfoming the complete CR flow. Now, I would like to perform a feedback or a meta analysis session with the goal of improving the agent docs, so that the next agent playing the <role> role can do a better job. Here is some things I have in my mind:

-<points you would like to improve>

Where do you think the documents should be improved so that the <role> agent can perform its task effectively?`

# Models

Initially I was using Gemini 3, switched to Gemini 3 flash. It felt like flash was better than other gemini models which were slow and didn't always produce the best result. I have had the best expirience with Opus 4.5, follows the agent docs better that every other model.

# Control