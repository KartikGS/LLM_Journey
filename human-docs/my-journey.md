# My journey

If you have read the [project readme](/README.md), you know that this poject aims to explore the advancements made in AI industry, from transformer models to agentic system. A lot happened in 2025 in different aspects of the AI industry, but in my opinion the most prominant advancement was made in the Agentic Stack, with Coding Agents leading the way.
This project was a way for me to catch to all the progress made. I installed Cursor in Dec 2025 (Yes I know, I am lightyears behind) and used an old repository that was supposed to be something else (don't ask) and started builing.

I had a rough idea about what this project was going to look like when it started, a few pages that will implement various concepts like tool calling, mcps, agentic workflow, etc  with rough explanation. Implemented a basic home page and a transformers page where a tiny decoder only transformer model (from a Andrej Karpathy vid) hosted using ONNX runtine with the help of the cursor agent and tiny polish here and there from me.

After seeing a few tweets (from Santiago) talking about how most projects using agents will ship garbage because these projects are either not observable or not writing tests, I though lets include these in the project early so that agents have a strong base to build from. I though this was going to be a one week side mission, after which I will continue building the actual website. Boy I was wrong, I went from trying to implement client side trace from scratch (as recommended by LLMs, reasoning that it will provide more "control"), reviewing the code generated by agent, putting the code changes in chatgpt for review, debugging, reviewing again, using chatgpt to review again, to finally admitting defeat.

(Side Note: I hit my token limits with cursor, switched to google antigravity. Claude code had a lot of hype but I couldn't pass up on the plan google was offering)

# Realization

I was expecting too much from these coding agents. Even thought the project was small, the agent had to figure out what this project was about on every new request. I didn't really know how to get the best out these agents, the only thing I ever prompted was the requirements.
After struggling to implement observability from scratch, I (with the help of ChatGPT) decided to introduce observablility using OpenTelemetry instrumentation and store the data in Grafana. I decided to do this on two parts: Client side observability and Server side observability. Even after the segragating the task, implemetation was a struggle (I will have to make a separate doc for the head banging journey I went through here).

For agentic coding to work, following good software development practices is not enough, you also need structured instructions for the agents to follow. Documentation that tell agents how to work. Agents will make the same mistakes again and again, there needs to be a record of this, so that agents can avoid repeating mistakes. I heard about AGENTS.md (thanks to 'this day in ai' pod) and thought maybe this is what I was missing. This is when I decided to make this project agent friendly and started working on the agent docs.

# Agent Docs

The ideas was to have documentation that would make the agents job easier. I wanted separate agents to handle somewhat independent aspects of the project. This resulted in the creation of the subagents, that handle different aspects of the project like frontend, backend, etc. I also wanted a agent that would help clarify the requirement/specification so that the task to be implemented would be crystal clear and no assumptions would be made during the development. This resulted in the creation of the BA agent. Since this replicated real world teams, I created the tech lead to coordinate the sub-agents.

# Feedback / Meta Analysis

The agent docs and workflow became better over time by carrying out feedback or meta analysis after agent performed their task. Below is an example of the feedback prompts I used (I didn't refine these prompts, just didn't feel like the right thing to do):

"""

Thanks for playing the role of the {role} Agent. Good job on perfoming the complete CR flow. Now, I would like to perform a feedback or a meta analysis session with the goal of improving the agent docs, so that the next agent playing the {role} role can do a better job. Here is some things I have in my mind:

- List of issues and inconsistencies you noticed

Where do you think the documents should be improved so that the {role} agent can perform its task effectively?

"""

# Models

Initially I was using Gemini 3 pro, switched to Gemini 3 flash. It felt like Flash was better than Gemini 3 Pro, which were slow and didn't always produce the best result. I have had the best expirience with Opus 4.5, follows the agent docs better that every other model (Seriously considering switching to Claude Code).

# Control

Initially I had a rough vision of what this project was going to look like but it was not concrete. I wanted to set a clear vision for this project so that there is no ambiguity when major development starts. After bouncing around idea with multiple llms, a concrete project vision was set. I liked the new structure that this project had. But when the time came to start developing these ideas, I couldn't really think or visualize anything. It seemed like I didn't really have a concrete picture of what the website would look like, anymore. This project went from being my idea to idea generated by something else, which is somewhat hard to adopt.

# Implementation