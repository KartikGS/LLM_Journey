BASE_URL= 'http://localhost:3001'

# OpenTelemetry Configuration
## Endpoint of the otel collector 
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
## Authorization header for the otel collector
OTEL_EXPORTER_OTLP_HEADERS='Authorization=Bearer change_me_in_production'
## secret for telemetry token creation
TELEMETRY_SECRET= 'default_secret_change_me_in_production'
# E2E Testing
PLAYWRIGHT_BASE_URL='http://localhost:3001'

# Frontier Base Inference (server-side only)
# Provider format: 'openai' (default) or 'huggingface'
# - openai: OpenAI-compatible chat completions endpoint
# - huggingface: HF Inference API (https://api-inference.huggingface.co/models/{model_id})
FRONTIER_PROVIDER='openai'
# API endpoint URL for the selected provider
# OpenAI example:      https://api.openai.com/v1/chat/completions
# HuggingFace example: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B
FRONTIER_API_URL='https://api.openai.com/v1/chat/completions'
# Base (non-assistant-fine-tuned) model identifier
FRONTIER_MODEL_ID='replace_with_base_model_id'
# Provider API key (secret; do not expose client-side)
FRONTIER_API_KEY='replace_with_frontier_api_key'
# Optional timeout in milliseconds (bounded to 1000-20000, default 8000)
FRONTIER_TIMEOUT_MS='8000'
